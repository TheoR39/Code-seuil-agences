{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f4d864-c4f0-478b-a991-f3c0d78b0dec",
   "metadata": {},
   "source": [
    "# Clustering des agences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f7ece-4c58-4c31-9f8e-85ca2ef60a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import squarify\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from Code_analyse_OOP import DataCharger, BasicStats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Optional, int, bool\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE \n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e671662-6e8b-4fac-9e1d-0687156b8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering_agences:\n",
    "    # On suppose que complete_data contient les données de toutes les agences sur 2024\n",
    "    # On suppose également que 'code_agence' est mis en index\n",
    "    # On suppose enfin que complete_data a été nettoyé au préalable avec PreprocessingRawData\n",
    "    # Dans un premier temps, on a besoin de créer un nouveau DataFrame avec les features retenue\n",
    "    def __init__(self, new_filepath, filepath = None):\n",
    "        self.filepath = filepath  # Filepath de la donnée complète\n",
    "        self.new_filepath = new_filepath\n",
    "        self.data = None  # On commence par créer un DataFrame vide\n",
    "        self.data_scaled = None\n",
    "        self.best_nb_k_means = None\n",
    "        self.pca_object = None\n",
    "        self.applied_pca = None\n",
    "\n",
    "    def remplissage_data(self):  # Devrait permettre de créer la donnée nécessaire à passer aux algos\n",
    "        dataset = DataCharger(self.filepath)\n",
    "        liste_agences = dataset.index.tolist()\n",
    "        lignes  = []\n",
    "        for agence in liste_agences:\n",
    "            class_data = DataCharger(self.filepath, agence, 2024)\n",
    "            data_agence = BasicStats(class_data)\n",
    "            lignes.append(data_agence.data_retrieval_clustering())\n",
    "        self.data = pd.DataFrame(lignes)\n",
    "        self.data.set_index(\"code_agence\", inplace = True)\n",
    "        return self.data\n",
    "    \n",
    "    def save_data(self, new_filepath : int): # Pour enregistrer une bonne fois pour toutes\n",
    "        self.data.to_csv(new_filepath, index = True)\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.new_filepath, index_col = 0)\n",
    "\n",
    "    # A partir d'ici, on suppose disposer de la donnée adéquate pour lancer un algorithme de clustering\n",
    "    # Cad : on suppose disposer d'un dataframe avec le code_agence en index et des features en colonne\n",
    "    # On risque cependant d'avoir à faire de la réduction de dimension pour éviter trop de problèmes...\n",
    "\n",
    "    def heatmap_features(self):\n",
    "        plt.figure(figsize = (15,15))\n",
    "        sns.heatmap(self.data.corr(), cmap = 'coolwarm', center = 0, annot = True, fmt = \".2f\")\n",
    "        plt.show()   # On visualise les corrélations potentielles pour éviter trop de features\n",
    "\n",
    "    def scaling_data(self):\n",
    "          scaler = StandardScaler()\n",
    "          data_scaled = scaler.fit_transform(self.data)\n",
    "          self.data_scaled = pd.DataFrame(data_scaled, columns = self.data.columns, index = self.data.index)\n",
    "\n",
    "    def remove_feature(self, column):\n",
    "        self.data = self.data.drop(columns = column)  # Si une feature est inutile\n",
    "\n",
    "    def apply_PCA(self, n_components : Optional[int] = None, var_kept : Optional[float] = None):\n",
    "        assert self.data_scaled is not None, \"Les données doivent impérativement être standardisées\"\n",
    "        if var_kept is not None:\n",
    "            pca = PCA(n_components = var_kept)\n",
    "        elif n_components is not None:\n",
    "            pca = PCA(n_components = n_components)\n",
    "        else:\n",
    "            raise ValueError(\"Au moins l'un des deux arguments 'n_components' ou 'var_kept' doit être non vide\")\n",
    "        reduced = pca.fit_transform(self.data_scaled)\n",
    "        column_names = [f\"PCA{i+1}\" for i in range(reduced.shape[1])]\n",
    "        self.applied_pca = pd.DataFrame(reduced, index = self.data.index, columns = column_names)\n",
    "        self.pca_object = pca\n",
    "        return self.applied_pca, self.pca_object\n",
    "\n",
    "\n",
    "# Méthodes pour clusteriser au sens de k-means:\n",
    "\n",
    "    def elbow_method_k_means(self):  # On cherche à déterminer le nombre optimal de clusters\n",
    "        inertias = []\n",
    "        K = range(1, 11)  # nombre de clusters à tester\n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(self.data_scaled)\n",
    "            inertias.append(kmeans.inertia_)  # inertia = somme des distances au centre du cluster\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(K, inertias, 'bo-')\n",
    "        plt.xlabel('Nombre de clusters')\n",
    "        plt.ylabel('Inertie (Within-Cluster Sum of Squares)')\n",
    "        plt.title(\"Méthode du coude pour déterminer le nombre optimal de clusters\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def silhouette_score_k_means(self):\n",
    "        silhouette_scores = []\n",
    "        assert self.data_scaled is not None, \"Il faut d'abord standardiser les données avec StandardScaler\"\n",
    "        for k in range(2, 11):  # 2 clusters minimum\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(self.data_scaled)\n",
    "            score = silhouette_score(self.data_scaled, labels)\n",
    "            silhouette_scores.append(score)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(2, 11), silhouette_scores, 'go-')\n",
    "        plt.xlabel('Nombre de clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.title(\"Score de silhouette en fonction du nombre de clusters\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def find_best_k(self, max_k=10):  # Fonction à vérifier...\n",
    "        silhouette_scores = []\n",
    "        inertias = []\n",
    "        for k in range(2, max_k + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(self.data_scaled)\n",
    "            silhouette_scores.append(silhouette_score(self.data_scaled, labels))\n",
    "            inertias.append(kmeans.inertia_)\n",
    "        best_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "        self.best_nb_k_means = best_k\n",
    "        print(f\"Meilleur nombre de clusters estimé (silhouette) : {best_k}\")\n",
    "        return best_k\n",
    "\n",
    "\n",
    "    def clustering_k_means(self, pca: Optional[bool] = False, random_state: Optional[int] = 42):\n",
    "        k = self.best_nb_k_means\n",
    "        assert k is not None, \"Définissez le nombre optimal de clusters à créer\"\n",
    "        kmeans_final = KMeans(n_clusters = k, random_state = random_state)\n",
    "        if not pca:\n",
    "            assert self.data_scaled is not None, \"Il faut d'abord standardiser les données avec StandardScaler\"\n",
    "            kmeans_final.fit(self.data_scaled)\n",
    "        else:\n",
    "            assert self.applied_pca is not None, \"Il faut d'abord appliquer la PCA aux données\"\n",
    "            kmeans_final.fit(self.applied_pca)\n",
    "        cluster_labels = kmeans_final.labels_\n",
    "        self.data[\"cluster_kmeans\"] = cluster_labels\n",
    "        group_cluster_kmeans = self.data.groupby(\"cluster_kmeans\")\n",
    "        for cluster_id, group in group_cluster_kmeans:  # renvoie une liste des clusters avec les agences\n",
    "            print(f\"\\nCluster {cluster_id}: \")\n",
    "            print(group.index.tolist())  \n",
    "        return group_cluster_kmeans\n",
    "    \n",
    "    def treemap_clusters_kmeans(self):\n",
    "        assert \"cluster_kmeans\" in self.data.columns, \"Il faut d'abord appliquer k-means\"\n",
    "        clusters = self.data.groupby(\"cluster_kmeans\")\n",
    "        sizes = [len(group) for _,group in clusters]\n",
    "        labels = [\n",
    "        f\"Cluster {cid}\\n\" + \"\\n\".join(group.index.astype(str).tolist())\n",
    "        for cid, group in clusters\n",
    "        ]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        squarify.plot(sizes=sizes, label=labels, alpha=0.8)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Treemap des clusters et agences associées\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Méthodes pour la clusterisation hiérarchique:\n",
    "\n",
    "    # La première méthode détermine la méthode la plus susceptible de donner des clusters adaptés\n",
    "    # Elle se base sur le calcul du coefficient de corrélation cophénétique\n",
    "    # Plus il est proche de 1, meilleure est la méthode\n",
    "    def find_best_method(self, pca : Optional[bool] = False):\n",
    "        data_used = self.applied_pca if pca else self.data_scaled\n",
    "        assert data_used is not None, \"Aucune donnée disponible pour clusteriser\"\n",
    "        methods = ['ward', 'complete', 'average', 'single']  # Méthodes possibles pour hierarchical clustering\n",
    "        best_method = None\n",
    "        best_score = -1  # Pire score de corrélation possible\n",
    "        for method in methods:\n",
    "            Z = linkage(data_used, method = method)\n",
    "            coph_corr,_ = cophenet(Z, pdist(data_used))\n",
    "            if coph_corr > best_score:\n",
    "                best_score = coph_corr\n",
    "                best_method = method\n",
    "        print(\"Meilleure méthode estimée par corrélation cophénétique: \", best_method)\n",
    "        print(\"Coefficient de corrélation cophénétique atteint par la méthode: \", round(best_score,4))\n",
    "        return best_method, best_score\n",
    "        \n",
    "\n",
    "    # La deuxième méthode calcule (automatiquement) le meilleur nombre de clusters en se basant sur le plus gros saut sur le dendrogramme\n",
    "    # En gros, à cet endroit, le dendrogramme fusionne des groupes très différents, donc on coupe avant.\n",
    "    def find_best_max_cluster(self, method : Optional[str] = \"ward\", pca : Optional[bool] = False, return_score : Optional[bool] = False):\n",
    "        data_used = self.applied_pca if pca else self.data_scaled\n",
    "        assert data_used is not None, \"Aucune donnée disponible pour clusteriser\"\n",
    "        Z = linkage(data_used, method = method)\n",
    "        distances = Z[:,2]\n",
    "        deltas = np.diff(distances)\n",
    "        index_max_jump  = np.argmax(deltas[-10:]) + len(deltas) - 10\n",
    "        threshold_distance = distances[index_max_jump + 1]\n",
    "        labels = fcluster(Z, t = threshold_distance, criterion = 'distance')\n",
    "        nb_clusters = len(set(labels))\n",
    "        print(f\"Nombre optimal estimé de clusters hiérarchiques: {nb_clusters}\")\n",
    "        score = silhouette_score(data_used, labels)\n",
    "        score = silhouette_score(data_used, labels) if return_score else None\n",
    "        if return_score:\n",
    "            print(\"Silhouette score: \", score)\n",
    "        return nb_clusters, labels, threshold_distance, score\n",
    "    \n",
    "\n",
    "    def select_method_cluster(self, pca : Optional[bool] = False):\n",
    "        best_method = self.find_best_method(pca)[0]\n",
    "        best_cluster = self.find_best_max_cluster(pca, method = best_method)[0]\n",
    "        return best_method, best_cluster\n",
    "    \n",
    "\n",
    "    def clustering_hierarchical(self, pca : Optional[bool] = False, method : str = 'ward', dendrogram : Optional[bool] = False,\n",
    "                                max_cluster : Optional[int] = None, best : Optional[bool] = False):\n",
    "        data_to_use = self.data_scaled\n",
    "        if pca:\n",
    "            data_to_use = self.applied_pca\n",
    "        assert data_to_use is not None\n",
    "        if best:\n",
    "            method, max_cluster = self.select_method_cluster()\n",
    "        Z = linkage(data_to_use, method = method)\n",
    "        if dendrogram:\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            dendrogram(Z, labels=self.data.index.tolist(), leaf_rotation=90)\n",
    "            plt.title(f\"Dendrogramme du clustering hiérarchique ({'PCA' if pca else 'Standard'}) - Méthode: {method}\")\n",
    "            plt.xlabel(\"Code agence\")\n",
    "            plt.ylabel(\"Distance\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        labels = None\n",
    "        if max_cluster:\n",
    "            labels = fcluster(Z, max_cluster, criterion='maxclust')\n",
    "            self.data[\"cluster_hierarchical\"] = labels\n",
    "            print(f\"\\nAgences regroupées en {max_cluster} clusters :\")\n",
    "            for cluster_id in range(1, max_cluster + 1):\n",
    "                membres = self.data[self.data[\"cluster_hierarchical\"] == cluster_id].index.tolist()\n",
    "                print(f\"Cluster {cluster_id} : {membres}\")\n",
    "        return labels\n",
    "    \n",
    "\n",
    "    def treemap_clusters_hierarchical(self):\n",
    "        assert \"cluster_hierarchical\" in self.data.columns, \"Il faut commencer par appliquer une méthode de hierarchical clustering\"\n",
    "        clusters = self.data.groupby(\"cluster_hierarchical\")\n",
    "        sizes = [len(group) for _, group in clusters]\n",
    "        labels = [\n",
    "        f\"Cluster {cid}\\n\" + \"\\n\".join(group.index.astype(str).tolist())\n",
    "        for cid, group in clusters\n",
    "        ]\n",
    "        plt.figure(figsize = (12,6))\n",
    "        squarify.plot(sizes = sizes, label = labels, alpha = 0.8)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Treemap des clusters hiérarchiques et de leurs agences\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Méthodes pour le clustering DBSCAN:\n",
    "\n",
    "    def DBSCAN(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Méthodes pour la visualisation (avec PCA, t-SNE):\n",
    "\n",
    "    def tsne_visual_clusters(self, cluster_col : str = \"cluster_kmeans\", pca : Optional[bool] = False,\n",
    "                             perplexity  : int = 15, random_state: Optional[int] = 42):\n",
    "        assert cluster_col in self.data.columns, f\"Il faut d'abord appliquer le clustering qui donne la colonne {cluster_col}\"\n",
    "        data_used = self.applied_pca if pca else self.data_scaled\n",
    "        assert data_used is not None, \"Il faut impérativement normaliser les données\"\n",
    "        tsne = TSNE(n_components = 2, perplexity = perplexity, random_state = random_state)\n",
    "        tsne_result = tsne.fit_transform(data_used)\n",
    "        df_tsne = pd.DataFrame(tsne_result, columns = ['TSNE1', 'TSNE2'], index = self.data.index)\n",
    "        df_tsne[cluster_col] = self.data[cluster_col]\n",
    "        plt.figure(figsize = (10,6))\n",
    "        sns.scatterplot(data = df_tsne, x='TSNE1', y='TSNE2', hue = cluster_col, palette = 'tab10', s=80)\n",
    "        plt.title(f\"Projection t-SNE des clusters ({cluster_col})\")\n",
    "        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "# Agrégation des méthodes de clustering:\n",
    "\n",
    "    def agreg_clustering(self):  # A compléter pour obtenir une méthode d'analyse\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4a42d",
   "metadata": {},
   "source": [
    "- Pour éviter de réitérer plusieurs fois la construction de la donnée, on peut, dans la fonction terminale, demander à l'utilisateur s'il dispose déjà de la donnée, auquel cas on évite d'appeler la méthode 'remplissage_data'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c1549",
   "metadata": {},
   "source": [
    "On doit commencer par trouver des paramètres pertinents qui permettront de créer des catégories d'agences par apprentissage non supervisé (algorithmes de clustering). On va devoir envisager plusieurs types de clusters (suivant les différents types de techniques, comme k-means, DBSCAN, hierarchical clustering...).\n",
    "- 1ère étape : Feature Engineering\n",
    "- 2ème étape : Construction des clusters\n",
    "- 3ème étape : Comparaison\n",
    "\n",
    "En particulier, ceci permettra de générer deux seuils par cluster, ce qui simplifie (considérablement) le problème, mais également d'accéder à plus de données. Ceci peut être particulièrement utile pour résoudre le problème d'optimisation, en s'intéressant au cluster entier et pas à chacune des agences individuellement. Notamment, on pourra plus facilement envisager d'utiliser des méthodes de bootstrap à partir des données réelles pour générer les échantillons nécessaires à l'application de la méthode SAA.\n",
    "\n",
    "* Autre possibilité : Développer un modèle de séries temporelles pour chaque cluster, en espérant qu'il arrive à capter les différentes tendances..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c1c29",
   "metadata": {},
   "source": [
    "1er problème : Trouver des caractéristiques (features) pertinentes pour permettre aux algos de clustering d'établir une classification précise.\n",
    "Dans un premier temps, au vu de l'exploration préalable des données, on conserve les paramètres suivants:\n",
    "   - flux nets (versements - retraits) de l'agence : plus facilement manipulable que retraits et versements séparés.\n",
    "   - fréquence des outliers de retraits : permet aussi d'estimer la proba qu'un outlier de retrait se présente\n",
    "   - Ecart-type (variabilité) des versements et des retraits : on a vu qu'elle pouvait varier suivant les agences et qu'elle pouvait ou pas contrebalancer les montants\n",
    "   - Nombre moyen de retraits / versements journaliers\n",
    "   - Moyenne (ou médiane) des retraits / versements journaliers\n",
    "   - Ecart-type (variabilité) du montant des retraits / versements journaliers\n",
    "   - Nombre de ruptures : nombre de jours où l'agence tombe en rupture sur une année par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764dd5bb",
   "metadata": {},
   "source": [
    "Pb. à gérer / idées :\n",
    "- Penser à utiliser plotly (dans un second temps) pour obtenir une visualisation interactive des clusters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
